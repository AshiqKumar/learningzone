# Computer Vision Theory - Mathematical Foundations

## ğŸ“ Mathematical Foundations of Computer Vision

### 1. Image Processing Fundamentals

#### 1.1 Digital Image Representation

A digital image is a 2D function I(x,y) where:
- (x,y) are spatial coordinates
- I(x,y) is the intensity value at position (x,y)
- For grayscale: I(x,y) âˆˆ [0, 255] (8-bit)
- For color: I(x,y) = [R(x,y), G(x,y), B(x,y)]

**Pixel Neighborhood**:
- **4-connectivity**: {(xÂ±1,y), (x,yÂ±1)}
- **8-connectivity**: {(xÂ±1,yÂ±1), (xÂ±1,y), (x,yÂ±1)}

#### 1.2 Convolution and Filtering

**2D Convolution**:
```
(I * K)(x,y) = Î£Î£ I(m,n) Ã— K(x-m, y-n)
               m n
```

Where:
- I: Input image
- K: Convolution kernel
- *: Convolution operator

**Properties**:
- Commutative: I * K = K * I
- Associative: (I * Kâ‚) * Kâ‚‚ = I * (Kâ‚ * Kâ‚‚)
- Linear: I * (aKâ‚ + bKâ‚‚) = a(I * Kâ‚) + b(I * Kâ‚‚)

**Common Kernels**:

*Gaussian Blur*:
```
G(x,y) = (1/2Ï€ÏƒÂ²) Ã— e^(-(xÂ²+yÂ²)/2ÏƒÂ²)
```

*Laplacian (Edge Detection)*:
```
[ 0  1  0]
[ 1 -4  1]
[ 0  1  0]
```

*Sobel X-direction*:
```
[-1  0  1]
[-2  0  2]
[-1  0  1]
```

### 2. Feature Detection Theory

#### 2.1 Edge Detection Mathematics

**Gradient-based Methods**:
- Gradient magnitude: |âˆ‡I| = âˆš((âˆ‚I/âˆ‚x)Â² + (âˆ‚I/âˆ‚y)Â²)
- Gradient direction: Î¸ = arctan(âˆ‚I/âˆ‚y / âˆ‚I/âˆ‚x)

**Canny Edge Detection Steps**:
1. **Gaussian Smoothing**: I_smooth = I * G_Ïƒ
2. **Gradient Calculation**: G_x, G_y using Sobel operators
3. **Non-maximum Suppression**: Keep only local maxima
4. **Double Thresholding**: T_low < edge < T_high
5. **Edge Linking**: Connect strong edges through weak edges

**Mathematical Framework**:
```
Step 1: Smoothing
I_smooth(x,y) = Î£ Î£ G(i,j,Ïƒ) Ã— I(x+i, y+j)

Step 2: Gradient
G_x = âˆ‚I_smooth/âˆ‚x, G_y = âˆ‚I_smooth/âˆ‚y
Magnitude = âˆš(G_xÂ² + G_yÂ²)
Direction = arctan2(G_y, G_x)

Step 3: Non-maximum suppression
For each pixel, check if it's local maximum along gradient direction
```

#### 2.2 Corner Detection Theory

**Harris Corner Detector**:

The second moment matrix (structure tensor):
```
M = [Î£(I_xÂ²)    Î£(I_xÃ—I_y)]
    [Î£(I_xÃ—I_y) Î£(I_yÂ²)   ]
```

Where summation is over local window W.

**Eigenvalue Analysis**:
- Î»â‚, Î»â‚‚ are eigenvalues of M
- Î»â‚ â‰ˆ Î»â‚‚ â‰ˆ 0: uniform region
- Î»â‚ >> Î»â‚‚ â‰ˆ 0: edge
- Î»â‚ â‰ˆ Î»â‚‚ >> 0: corner

**Harris Response**:
```
R = det(M) - k Ã— (trace(M))Â²
R = Î»â‚Î»â‚‚ - k(Î»â‚ + Î»â‚‚)Â²

Where k âˆˆ [0.04, 0.06]
```

**Corner Condition**:
- R > threshold: corner
- R < -threshold: edge
- |R| < threshold: uniform

### 3. Optical Flow Theory

#### 3.1 Brightness Constancy Assumption

**Fundamental Equation**:
```
I(x, y, t) = I(x + dx, y + dy, t + dt)
```

**Taylor Expansion**:
```
I(x+dx, y+dy, t+dt) â‰ˆ I(x,y,t) + (âˆ‚I/âˆ‚x)dx + (âˆ‚I/âˆ‚y)dy + (âˆ‚I/âˆ‚t)dt
```

**Optical Flow Constraint**:
```
I_x Ã— u + I_y Ã— v + I_t = 0
```

Where:
- (u,v) = (dx/dt, dy/dt) is optical flow vector
- I_x, I_y, I_t are partial derivatives

#### 3.2 Lucas-Kanade Method

**Problem**: One equation, two unknowns (aperture problem)

**Solution**: Use local window W and assume constant flow

**System of Equations**:
```
For each pixel (i,j) in window W:
I_x(i,j) Ã— u + I_y(i,j) Ã— v = -I_t(i,j)
```

**Matrix Form**:
```
[I_x(1,1)  I_y(1,1)]       [-I_t(1,1)]
[I_x(1,2)  I_y(1,2)]  [u] = [-I_t(1,2)]
[   ...       ...   ]  [v]   [   ...  ]
[I_x(m,n)  I_y(m,n)]       [-I_t(m,n)]

A Ã— v = b
```

**Least Squares Solution**:
```
v = (A^T A)^(-1) A^T b

Where A^T A = [Î£(I_xÂ²)    Î£(I_x I_y)]
              [Î£(I_x I_y) Î£(I_yÂ²)  ]
```

### 4. Geometric Transformations

#### 4.1 Homogeneous Coordinates

**2D Point Representation**:
```
Cartesian: (x, y)
Homogeneous: [x, y, 1]^T or [wx, wy, w]^T
```

**Transformation Types**:

*Translation*:
```
[x']   [1  0  t_x] [x]
[y'] = [0  1  t_y] [y]
[1 ]   [0  0   1 ] [1]
```

*Rotation*:
```
[x']   [cos(Î¸) -sin(Î¸)  0] [x]
[y'] = [sin(Î¸)  cos(Î¸)  0] [y]
[1 ]   [  0       0     1] [1]
```

*Scaling*:
```
[x']   [s_x  0   0] [x]
[y'] = [ 0  s_y  0] [y]
[1 ]   [ 0   0   1] [1]
```

*General Affine*:
```
[x']   [a  b  c] [x]
[y'] = [d  e  f] [y]
[1 ]   [0  0  1] [1]
```

#### 4.2 Projective Transformations (Homography)

**Full Projective Transformation**:
```
[x']   [hâ‚â‚ hâ‚â‚‚ hâ‚â‚ƒ] [x]
[y'] = [hâ‚‚â‚ hâ‚‚â‚‚ hâ‚‚â‚ƒ] [y]
[w']   [hâ‚ƒâ‚ hâ‚ƒâ‚‚ hâ‚ƒâ‚ƒ] [1]
```

**Normalized Form**:
```
x' = (hâ‚â‚x + hâ‚â‚‚y + hâ‚â‚ƒ) / (hâ‚ƒâ‚x + hâ‚ƒâ‚‚y + hâ‚ƒâ‚ƒ)
y' = (hâ‚‚â‚x + hâ‚‚â‚‚y + hâ‚‚â‚ƒ) / (hâ‚ƒâ‚x + hâ‚ƒâ‚‚y + hâ‚ƒâ‚ƒ)
```

### 5. Statistical Methods

#### 5.1 RANSAC (Random Sample Consensus)

**Algorithm Steps**:
1. Randomly select minimum sample set
2. Fit model to sample
3. Count inliers (points within threshold)
4. Repeat for N iterations
5. Return model with most inliers

**Mathematical Framework**:
- Probability of outlier: Îµ
- Sample size: s
- Probability of good sample: (1-Îµ)^s
- Probability of failure in N iterations: [1-(1-Îµ)^s]^N
- Required iterations: N = log(Î·) / log(1-(1-Îµ)^s)

Where Î· is desired probability of success.

#### 5.2 Probability Distributions in Vision

**Gaussian Distribution**:
```
N(x; Î¼, ÏƒÂ²) = (1/âˆš(2Ï€ÏƒÂ²)) Ã— exp(-(x-Î¼)Â²/2ÏƒÂ²)
```

**Multivariate Gaussian**:
```
N(x; Î¼, Î£) = 1/âˆš((2Ï€)^k|Î£|) Ã— exp(-Â½(x-Î¼)^T Î£^(-1) (x-Î¼))
```

**Applications**:
- Noise modeling in images
- Feature descriptor distributions
- Kalman filter state estimation
- Gaussian mixture models for background subtraction

### 6. Information Theory in Vision

#### 6.1 Entropy and Information

**Shannon Entropy**:
```
H(X) = -Î£ P(x) logâ‚‚ P(x)
```

**Mutual Information**:
```
I(X;Y) = Î£ Î£ P(x,y) logâ‚‚(P(x,y)/(P(x)P(y)))
```

**Applications**:
- Image registration (maximize mutual information)
- Feature selection (maximum information gain)
- Compression and encoding

#### 6.2 Histogram Analysis

**Image Histogram**:
```
h(i) = number of pixels with intensity i
```

**Normalized Histogram**:
```
p(i) = h(i) / (total number of pixels)
```

**Histogram Equalization**:
```
s = T(r) = (L-1) Ã— Î£(k=0 to r) p(k)
```

Where:
- r: input intensity level
- s: output intensity level  
- L: number of intensity levels (256 for 8-bit)

### 7. Linear Algebra in Computer Vision

#### 7.1 Singular Value Decomposition (SVD)

For matrix A (mÃ—n):
```
A = UÎ£V^T
```

Where:
- U (mÃ—m): left singular vectors
- Î£ (mÃ—n): diagonal matrix of singular values
- V (nÃ—n): right singular vectors

**Applications**:
- Principal Component Analysis (PCA)
- Fundamental matrix estimation
- Low-rank approximations

#### 7.2 Eigenvalue Decomposition

For symmetric matrix A:
```
A = QÎ›Q^T
```

Where:
- Q: matrix of eigenvectors
- Î›: diagonal matrix of eigenvalues

**Applications**:
- Covariance analysis
- Principal component analysis
- Corner detection (Harris matrix)

### 8. Optimization in Computer Vision

#### 8.1 Least Squares Problems

**Linear Least Squares**:
```
min ||Ax - b||Â²
x

Solution: x = (A^T A)^(-1) A^T b
```

**Weighted Least Squares**:
```
min ||W(Ax - b)||Â²
x

Solution: x = (A^T W^T W A)^(-1) A^T W^T W b
```

#### 8.2 Non-linear Optimization

**Gauss-Newton Method**:
For f(x) = Â½||r(x)||Â²:
```
x_{k+1} = x_k - (J^T J)^(-1) J^T r(x_k)
```

Where J is Jacobian of residuals r(x).

**Levenberg-Marquardt**:
```
x_{k+1} = x_k - (J^T J + Î»I)^(-1) J^T r(x_k)
```

**Applications**:
- Bundle adjustment
- Camera calibration
- Non-linear shape fitting

### 9. Multi-scale Analysis

#### 9.1 Scale-Space Theory

**Gaussian Scale Space**:
```
L(x, y, Ïƒ) = G(x, y, Ïƒ) * I(x, y)
```

Where G(x,y,Ïƒ) is Gaussian kernel with standard deviation Ïƒ.

**Properties**:
- Causality: No new structures created at coarser scales
- Scale invariance: Natural parameterization
- Linearity: Superposition principle

#### 9.2 Image Pyramids

**Gaussian Pyramid**:
- Gâ‚€ = original image
- Gáµ¢ = REDUCE(Gáµ¢â‚‹â‚) = downsample after Gaussian blur

**Laplacian Pyramid**:
- Lâ‚€ = Gâ‚€ - EXPAND(Gâ‚)
- Láµ¢ = Gáµ¢ - EXPAND(Gáµ¢â‚Šâ‚)

**Applications**:
- Multi-resolution analysis
- Image compression
- Feature detection at multiple scales

---

This mathematical foundation provides the theoretical background needed to understand and implement the computer vision algorithms in the main codebase.
